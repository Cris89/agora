\chapter{Introduction}

\section{Problem and Motivations}

\lettrine{N}{}\textit{owadays}, increasingly accurate climate forecasts, biomedical researches, business analytics, astrophysics studies and, in general, Big Data problems require huge computing performance in order to obtain significant results. For these reasons, High Performance Computing (HPC) technologies have been continuously sped up and, now, their next target is to reach the Exascale level, that is the capability of at least one exaFLOPS, equal to a billion billion FLOPS (FLoating point Operations Per Second). This is the order of processing power of human brain at neural level, based on H. Markram et al. research study (\cite{markram2011introducing}).

In order to raise computing performance, the multicore scaling era has brought, over time, a constant frequency and power increase. However, it is impossible to follow this trend anymore, due to the beginning of dark silicon era (\cite{esmaeilzadeh2011dark}) and the end of Dennard scaling (\cite{dennard1974design}). In 1974, Robert H. Dennard provided a more granular view of the famous Moore's Law (\cite{moore1998cramming}) expressing the doubling of number of transistors, in a dense integrated circuit, approximately every two years. This trend produces faster processors because, as transistors get smaller, their power density is constant, so that power use is proportional with area. As transistors get smaller, so necessarily do voltage and current, giving the possibility to increase frequency. The ability to drop voltage and current, in order to let transistors operate reliably, has broken down. Static power (power consumed when transistor is not switching) has increased its contribution in overall power consumption, with an importance similar to dynamic power (power consumed when transistor changes its value), producing serious thermal issues and realistic breakdowns. This scenario implicates the impossibility to power-on all components of a circuit at nominal operating voltage due to Thermal Design Power (TDP) constraints.

Due to these physical problems, system energy efficiency has become essential. Even if, every approximately 1.5 years computation per kilowatt-hour have doubled over time (\cite{koomey2011implications}), Subramaniam et al. (\cite{subramaniam2013trends}) suggest that there is the need of an energy efficiency improvement at a higher rate than current trends, in order to reach target consumed power of 20 MW for Exascale systems, established by DARPA report (\cite{bergman2008exascale}).

Efficiency has become very important also for the electricity consumption of data centers: just the US data centers are expected to consume 140 billion kWh in 2020, from 61 billion kWh in 2006 and 91 billion kWh in 2013 (\cite{site:NRDC2015}), given that the amount of information managed by worldwide data centers is going to grow 50-fold while the number of total servers is going to increase only 10-fold (\cite{gantz2011extracting}).

It is straightforward that green and heterogeneous approaches have to be applied in order to improve general efficiency of High Performance Computing architectures in which multiple Central Processing Units (CPUs), General-Purpose computing on Graphics Processing Units (GPGPUs) and Many Integrated Cores accelerators (MICs) coexist and work together in a parallel way; for these reasons, \textit{TOP500 Green Supercomputers} (\cite{site:topGreen500}) demonstrates the large interest in green architectures, ranking and updating world top 500 supercomputers by energy efficiency.

There exist various approaches in order to deal with these problems, from both hardware and software point of view. Power Consumption Management consists of various techniques such as Dynamic Voltage and Frequency Scaling (DVFS) and ther\-mal-aware or ther\-mal-management technologies in order to deal with dark silicon issues (\cite{mittal2014power}). Another important concept is Approximate Computing (\cite{mittal2016survey}), that focuses on balancing computation quality and expended effort, both at programming level and in different processing units. This thesis deals with approximate techniques, exploited at software level, oriented to tunable High Performance Computing applications that follow the so called \textit{Autonomic Computing} research area (\cite{kephart2003vision}, \cite{computing2006architectural}). The underlying structure on which this theory is based is the Monitor-Analyze-Plan-Execute feedback loop, where systems have the capability to manage themselves, given high-level objectives and application knowledge in terms of possible configurations, made by parameter values (such as number of threads or processes involved, possible various kind of compiler optimization levels, application-specific variable values, etc.) and the corresponding metrics of interest (such as power consumption, throughput, output error with respect to a target value, etc.). This information represents the application design space.

For these kind of programs, the large number of parameters and their corresponding set of values make the design space huge and, in general, computation time is not negligible. Therefore, an exhaustive search is practically unfeasible, so there exist Design Space Exploration techniques that aim to provide approximated Pareto points, namely those configurations that solve better than others a typical multi-objective optimization problem (for instance, keeping throughput above some value while limiting output error and power consumption).

Typically, application knowledge is built at design-time. Then, it is passed to a dynamic autotuner that has the ability to choose, from time to time, the best possible set of parameters (also called \textit{dynamic knobs}) with respect to current goals and requirements that might change during execution.

One of the leading projects in this research area is ANTAREX (\cite{silvano2016antarex}), lead by Politecnico di Milano, that aspires to express by LARA (\cite{cardoso2012lara, cardoso2014performance}), a Domain Specific Language (DSL), application self-adaptivity and to autotune, at runtime, applications oriented to green and heterogeneous HPC systems, up to the Exascale level.





\section{Objectives}

Main objective of this work is to explore the application Design Space Exploration at runtime, collecting all information about used parameter values and related performance in terms of associated metrics of interest. Through this data and Machine Learning techniques, we aim to predict an application complete model, made by the whole possible configuration list. Finally, the model is used by the dynamic online autotuner, that can set up the related program execution in the best way, according to current goals and requirements.

To reach these objectives, we have developed in this thesis a framework, \textit{Agora}, that can manage multiple tunable applications that run in a parallel architecture; we want not only to simultaneously supervise different programs, but also to share Design Space Exploration phase among all nodes that run the same application, hence speeding up this process.

Any kind of prior offline phase and design-time knowledge, that separates applications from their execution in runtime autotuning mode, is therefore avoided. Agora makes autotuning completely independent from both application type and technical specifications about the machine in which program is executed. Agora does not need to know this information before it starts and final complete model is suitable regardless autotuner objectives.





\section{Thesis structure}

This thesis is structured as follow: in Chapter \ref{back} we focus on the background concepts related to our work, clarifying what is a parallel architecture, the meaning of \textit{Design Space Exploration} and \textit{Design of Experiments}, the concept of \textit{Dynamic Autotuning} and, finally, we present the MQTT messaging protocol and the Generalized Linear Regression (GLR) approach for application model prediction, used in our framework. Chapter \ref{sota} gives an overview of the State-of-the-Art about the principal methodologies related to Agora, in particular on current research studies about Design Space Exploration and Autotuning. In Chapter \ref{methodology} we address our intent, describing the overall context, our suggested methodology and tool, its general description and main strenghts. Chapter \ref{agora} shows the technical implementation of Agora use cases, related workflow and framework behavior. In Chapter \ref{exps} we collected some experimental results we have done in order to validate our approach and to highlight overall benefits. Finally, Chapter \ref{end} summarizes the entire work, the obtained results and possible future Agora developments.
