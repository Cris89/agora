\chapter{State-of-the-Art}\label{sota}

\lettrine{W}{}\textit{e} introduce main research studies that have represented a starting point for the design and implementation of A\-go\-ra. We divide them in two categories: Design Space Exploration and Autotuning.

\section{Design Space Exploration related works}

ReSPIR (\cite{palermo2009respir}) proposes a DSE methodology for application-spe\-cif\-ic multiprocessor systems-on-chip (MPSoCs). First, a Design of Experiments phase is used to capture an initial plan of experiments that represent the entire target Design Space to be explored by simulations. After that, Response Surface Methodology techniques (\cite{khuri2010response}) identify the feasible solution area with respect to the system-level constraints, in order to refine the Pareto configurations until a predetermined criterion is satisfied.

MULTICUBE Explorer (\cite{silvano2011multicube}) is an open source, portable, automatic Multi-Objective Design Space Exploration framework for tuning multi-core architectures. A designer, through an existing executable model (use case simulator) and a Design Space definition through a XML file, can explore his parametric architecture.

The $\epsilon$-Pareto Active Learning ($\epsilon$-PAL, \cite{zuluaga2016e}) aims at efficiently localize an $\epsilon$-accurate Pareto frontier in Multi-Objective Optimization problems. It models objectives as Gaussian process models, in order to guide the iterative design evaluation and, therefore, to maximize progress on those configurations that are likely to be Pareto optimal.

ReSPIR and MULTICUBE are researches oriented on ap\-pli\-ca\-tion-specific architecture design, while $\epsilon$-PAL deals with the MOO problem in a general way. All these three works aim to obtain a Pareto front and their execution is done offline. Agora uses the concept of Design Space Exploration but it is focused on Approximate Computing software strategies in executing applications. Agora does not calculate Pareto frontier, but its goal is to provide complete application model through Machine Learning techniques. Moreover, with respect to ReSPIR, MULTICUBE and $\epsilon$-PAL, we want to avoid any offline DSE phase, driving it during programs execution.

Furthermore, Agora is able to fulfill application Design Space Exploration in a shared way, among simultaneously running applications; with this improvement, DSE execution time is considerably reduced. 

\section{Autotuning related works}

SiblingRivalry (\cite{ansel2012siblingrivalry}) proposes an always online autotuning framework that uses evolutionary tuning techniques (\cite{coello2007evolutionary}) in order to adapt parallel programs. It eliminates the offline learning step: it divides available processor resources in half and it selects two configurations, a "safe" one and an "experimental" one, according to an internal fitness function value. After that, the online learner handles the identical current request in parallel on each half and, according to the candidate that finishes first and meets target goals and requirements, it updates its internal knowledge about configurations just used for current objective functions. This technique is used until a convergence is reached or when the context changes and, therefore, new objectives have to be achieved. When objectives change, SiblingRivalry restarts its procedure until new result is obtained. Agora doesn't ground its worklow on predetermined objectives; it is completely uninteresting about application goals and requirements, since it predicts the complete model for all metrics under examination. Furthermore, computational power of the machine in which the tunable program is executed is not kept busy by Agora, since application information gathering and model prediction are done remotely, on a different computer.

Capri framework (\cite{sui2016proactive}) focuses on control problem for tunable applications which mostly use Approximate Computing (\cite{mittal2016survey}) as improvement technique. Given an acceptable output error, it tries to minimize computational costs, such as running time or energy consumption. There are two main phases: training, done offline, estimates error function and cost function, using Machine Learning techniques with a training set of inputs; the control algorithm, done online, finds the best knob setting that fulfills objectives. Also Agora is oriented in tuning applications based on Approximate Computing techniques. It is focused on everything that precedes the control phase, supplying, to the dynamic online autotuner, metric of interest estimations for each possible application configuration. Agora wants to eliminate any offline phase, giving the possibility to simply run an application, driving its execution in order to collect a training set for model prediction; finally, the result is transmitted to application autotuner, that is in charge of managing the control phase.

mARGOt (\cite{gadioli2015application}) proposes a lightweight approach to application runtime autotuning for multicore architectures. It is based on the Monitor-Analyze-Plan-Execute (MAPE) feedback loop (\cite{kephart2003vision}), made up of a monitor infrastructure and an Application-Specific RunTime Manager (AS-RTM), based on Barbeque \hbox{(\cite{bellasi2012rtrm})}: the former element captures runtime information that is used by the latter in order to tune application knobs, together with design-time knowledge and application multi-objective requirements. The user has to provide XML configuration files in which a list of mARGOt Operating Points, made by parameter values and related metric of interest values, desired monitors and application objectives are expressed. The AS-RTM module, starting from this design-time knowledge and evaluating runtime information, has the task of choosing, from time to time, the best application configuration that satisfies application goals and requirements as best as possible. This work represents the starting point for the conception of Agora framework. mARGOt has to have the list of program Operating Points before execution; we want to produce this information while applications are running, removing to users the burden to make an offline step before taking advantage of autotuner capabilities.

There are other different autotuning frameworks in HPC context, yet based on design-time knowledge: 

\begin{itemize}

	\item OpenTuner (\cite{ansel2014opentuner}) builds up domain-specific program autotuners in which users can specify multiple objectives;
	
	\item ATune-IL (\cite{schaefer2009atune}) is an offline autotuner that gives the possibility to specify a wide range of tunable parameters for a general-purpose parallel program;

	\item PetaBricks (\cite{ansel2009petabricks}) is oriented on the definition of multiple algorithm implementations to solve a problem;

	\item Ananta Tiwari et al. (\cite{tiwari2009scalable}) propose a scalable and general-purpose framework for autotuning compiler-generated code, combining Active Harmony's parallel search backend (\cite{chung2004using}) and the CHiLL compiler transformation framework (\cite{chen2008chill});

	\item Green (\cite{baek2010green}) is focused on the energy-conscious programming using controlled approximation;

	\item PowerDial (\cite{hoffmann2011dynamic}) is a system that transforms static configuration parameters into dynamic knobs in order to adapt application behavior with respect to the accuracy of computation and the amount of resources.

\end{itemize}

Agora main difference is the absence of any kind of prior information about application features and the indifference on quality and quantity of metrics and objectives, in contrast with the last two mentioned works (\cite{baek2010green} and \cite{hoffmann2011dynamic}), focused on energy and accuracy goals.

Finally, among libraries for specific tasks, we can mention:

\begin{itemize}

	\item OSKI (\cite{vuduc2005oski}): it provides a collection of low-level primitives that automatically tunes computational kernels on sparse matrices;

	\item SPIRAL (\cite{puschel2004spiral}): it generates fast software implementations of linear signal processing transforms;

	\item ATLAS (\cite{whaley1998automatically}): it builds up a methodology for the automatic generation of basic linear algebra operations, focusing on matrix multiplications.

\end{itemize}

Agora aims to generalize autotuning process.
